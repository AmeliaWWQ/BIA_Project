{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8198399,"sourceType":"datasetVersion","datasetId":4833042},{"sourceId":8199175,"sourceType":"datasetVersion","datasetId":4772419}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom skimage import io\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nimport torchmetrics\nimport random","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-04-17T21:35:36.240164900Z","start_time":"2024-04-17T21:35:32.482004100Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-23T13:48:46.367173Z","iopub.execute_input":"2024-04-23T13:48:46.367518Z","iopub.status.idle":"2024-04-23T13:48:56.005980Z","shell.execute_reply.started":"2024-04-23T13:48:46.367488Z","shell.execute_reply":"2024-04-23T13:48:56.004983Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class GlauDataset(Dataset):\n    def __init__(self, is_train, transform=None):\n        self.is_train = is_train\n        self.transform = transform\n        print(\"AHAHA\")\n        \n        csv_file = '/kaggle/input/claheprocessed/path_specified_label_SIMPLIFIED.csv'\n        df = pd.read_csv(csv_file)\n        ids = df['Eye ID'].tolist()\n        labels = df['Final Label'].tolist()\n        ids = [_.split('/')[-1] for _ in ids]\n\n        pos_cases = []\n        neg = []\n        for id, lab in zip(ids, labels):\n            if lab == 1:\n                pos_cases.append((id, lab))\n            elif lab == 0:\n                neg.append((id, lab))\n            \n        # reduce the number of postitive cases\n        # pos_cases = pos_cases[:int(0.1*len(pos_cases))]\n        random.seed(100)\n        #print(len(pos_cases))\n        #print(len(neg_cases))\n        neg_cases = random.sample(neg, 3000)\n        num_pos_tr = int(len(pos_cases) * 0.8)\n        num_neg_tr = int(len(neg_cases) * 0.8)\n        self.train_data = pos_cases[:num_pos_tr] + neg_cases[:num_neg_tr]\n        self.val_data = pos_cases[num_pos_tr:] + neg_cases[num_neg_tr:]\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train_data)\n        else:\n            return len(self.val_data)\n\n    def __getitem__(self, idx):\n        if self.is_train:\n            img_path, lab = self.train_data[idx]\n            img = io.imread('/kaggle/input/claheprocessed/clahe_all/clahe_all/' + img_path)\n            img = img.astype(np.float32)\n            #img = min_max_scale(img)\n            img = img.transpose(2, 0, 1)\n            img = torch.from_numpy(img)\n            img = self.transform(img)\n        else:\n            img_path, lab = self.val_data[idx]\n            img = io.imread('/kaggle/input/claheprocessed/clahe_all/clahe_all/' + img_path)\n            img = img.astype(np.float32)\n            #img = min_max_scale(img)\n            img = img.transpose(2, 0, 1)\n            img = torch.from_numpy(img)\n            img = self.transform(img)\n\n        lab = torch.tensor(lab, dtype=torch.float32)\n        return img, lab\n\nclass GlauDatasetBalance(Dataset):\n    def __init__(self, is_train, transform=None):\n        self.is_train = is_train\n        self.transform = transform\n        \n        #csv_file = '/kaggle/input/cropped-field-of-view/path_specified_label_SIMPLIFIED.csv'\n        csv_file = '/kaggle/input/claheprocessed/path_specified_label_SIMPLIFIED.csv';\n        df = pd.read_csv(csv_file)\n        ids = df['Eye ID'].tolist()\n        labels = df['Final Label'].tolist()\n        ids = [_.split('/')[-1] for _ in ids]\n\n        pos_cases = []\n        neg_cases = []\n        for id, lab in zip(ids, labels):\n            if lab == 1:\n                pos_cases.append((id, lab))\n            elif lab == 0:\n                neg_cases.append((id, lab))\n\n        num_pos_tr = int(len(pos_cases) * 0.8)\n        num_neg_tr = int(len(neg_cases) * 0.8)\n        self.train_data = pos_cases[:num_pos_tr]\n        self.train_data_neg = neg_cases[:num_neg_tr]\n        self.val_data = pos_cases[num_pos_tr:] + neg_cases[num_neg_tr:]\n\n        # Oversample positive cases to the number of training samples\n        factor = num_neg_tr // num_pos_tr\n        residue = num_neg_tr % num_pos_tr\n        self.train_data = self.train_data * factor + self.train_data[:residue]\n        \n    def __len__(self):\n        if self.is_train:\n            return len(self.train_data)  # number of positive samples for training\n        else:\n            return len(self.val_data)\n\n    def __getitem__(self, idx):\n        if self.is_train:\n            img_path, lab = self.train_data[idx]\n            img_path_n, lab_n = self.train_data_neg[idx]\n            \n            img = io.imread('/kaggle/input/claheprocessed/clahe_all/clahe_all/' + img_path)\n            img = img.astype(np.float32)\n            img = min_max_scale(img)\n            img = img.transpose(2, 0, 1)\n            img = torch.from_numpy(img)\n            img = self.transform(img)\n            \n            img_n = io.imread('/kaggle/input/claheprocessed/clahe_all/clahe_all/' + img_path_n)\n            img_n = img_n.astype(np.float32)\n            img_n = min_max_scale(img_n)\n            img_n = img_n.transpose(2, 0, 1)\n            img_n = torch.from_numpy(img_n)\n            img_n = self.transform(img_n)\n            \n            lab = torch.tensor(lab, dtype=torch.float32)\n            lab_n = torch.tensor(lab_n, dtype=torch.float32)\n            return img, img_n, lab, lab_n\n            \n        else:\n            img_path, lab = self.val_data[idx]\n            img = io.imread('/kaggle/input/claheprocessed/clahe_all/clahe_all/' + img_path)\n            img = img.astype(np.float32)\n            img = min_max_scale(img)\n            img = img.transpose(2, 0, 1)\n            img = torch.from_numpy(img)\n            img = self.transform(img)\n            \n            lab = torch.tensor(lab, dtype=torch.float32)\n            return img, lab\n\ndef min_max_scale(img):\n    min_val = img.min(axis=(0, 1))\n    max_val = img.max(axis=(0, 1))\n    img = (img - min_val)/(max_val-min_val+1e-8)\n    return img\n\nclass ResNet50(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.hub.load(\"pytorch/vision\", \"resnet50\")#, weights=\"IMAGENET1K_V2\")\n        self.model.fc = nn.Linear(2048, 1)\n\n    def forward(self, x):\n        return self.model(x).squeeze()\n    \nclass ResNet18(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.hub.load(\"pytorch/vision\", \"resnet18\", weights=\"ResNet18_Weights.IMAGENET1K_V1\")\n        self.model.fc = nn.Linear(512, 1)\n\n    def forward(self, x):\n        return self.model(x).squeeze()","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-04-17T21:35:43.035829300Z","start_time":"2024-04-17T21:35:43.032837500Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-23T13:48:56.007725Z","iopub.execute_input":"2024-04-23T13:48:56.008157Z","iopub.status.idle":"2024-04-23T13:48:56.037289Z","shell.execute_reply.started":"2024-04-23T13:48:56.008132Z","shell.execute_reply":"2024-04-23T13:48:56.036186Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#device = xm.xla_device()\nprint(device)\n\n# Set hyperparameters\nnum_epochs = 25\nbatch_size = 32\nlearning_rate = 0.00001\n\n# Balanced sampling\nis_balance = False\n\n# Initialize transformations for data augmentation\ntransform = transforms.Compose([\n    transforms.Resize((256, 256), antialias=True),\n    transforms.RandomHorizontalFlip(),\n    # transforms.RandomVerticalFlip(),\n    # transforms.RandomRotation(degrees=45),\n    # transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n    # transforms.CenterCrop(224),\n    # transforms.ToTensor(),\n    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntransform_val = transforms.Compose([\n    transforms.Resize((256, 256), antialias=True),\n    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","metadata":{"ExecuteTime":{"end_time":"2024-04-17T21:35:45.209511500Z","start_time":"2024-04-17T21:35:45.204524600Z"},"execution":{"iopub.status.busy":"2024-04-23T13:48:56.038422Z","iopub.execute_input":"2024-04-23T13:48:56.038775Z","iopub.status.idle":"2024-04-23T13:48:56.091335Z","shell.execute_reply.started":"2024-04-23T13:48:56.038743Z","shell.execute_reply":"2024-04-23T13:48:56.089039Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the ImageNet Object Localization Challenge dataset\n# train_dataset = torchvision.datasets.ImageFolder(\n#     root='/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train',\n#     transform=transform\n# )\nif is_balance:\n    train_dataset = GlauDatasetBalance(is_train=True, transform=transform)\n    val_dataset = GlauDatasetBalance(is_train=False, transform=transform_val)\nelse:\n    print(\"HERE\")\n    train_dataset = GlauDataset(is_train=True, transform=transform)\n    val_dataset = GlauDataset(is_train=False, transform=transform_val)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n# Load the ResNet model\n#model = ResNet50()\n#model = ResNet34()\nmodel = ResNet18()\n\n# Parallelize training across multiple GPUs\n# model = torch.nn.DataParallel(model)\n\n# Set the model to run on the device\nmodel = model.to(device)","metadata":{"ExecuteTime":{"end_time":"2024-04-17T21:35:51.490893800Z","start_time":"2024-04-17T21:35:48.180565100Z"},"execution":{"iopub.status.busy":"2024-04-23T13:48:56.094252Z","iopub.execute_input":"2024-04-23T13:48:56.094725Z","iopub.status.idle":"2024-04-23T13:48:59.238002Z","shell.execute_reply.started":"2024-04-23T13:48:56.094681Z","shell.execute_reply":"2024-04-23T13:48:59.237155Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"HERE\nAHAHA\nAHAHA\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /root/.cache/torch/hub/main.zip\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 139MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the loss function and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 5e-4)\n#scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer, total_iters=num_epochs, power=1.0)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma = 0.9, step_size = learning_rate)","metadata":{"ExecuteTime":{"end_time":"2024-04-17T21:35:53.343070900Z","start_time":"2024-04-17T21:35:53.331103200Z"},"execution":{"iopub.status.busy":"2024-04-23T13:48:59.239132Z","iopub.execute_input":"2024-04-23T13:48:59.239433Z","iopub.status.idle":"2024-04-23T13:48:59.245583Z","shell.execute_reply.started":"2024-04-23T13:48:59.239408Z","shell.execute_reply":"2024-04-23T13:48:59.244631Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Train the model...\nfor epoch in range(num_epochs):\n    # Metric\n    metric = torchmetrics.classification.BinaryAUROC()\n    metric_acc = torchmetrics.classification.BinaryAccuracy()\n    metric.to(device)\n    metric_acc.to(device)\n    log_loss = 0\n    num_it = 0\n    for samps in train_loader:\n        if is_balance:\n            inputs = torch.cat((samps[0], samps[1]), dim=0)\n            labels = torch.cat((samps[2], samps[3]), dim=0)\n        else:\n            inputs = samps[0]\n            labels = samps[1]\n        \n        # Move input and label tensors to the device\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # Zero out the optimizer\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        \n        # Log loss\n        log_loss += loss.item()\n        num_it += 1\n\n        metric.update(outputs, labels)\n        metric_acc.update(outputs, labels)\n\n    scheduler.step()\n    for param_group in optimizer.param_groups:\n        current_lr = param_group['lr']\n\n    auroc = metric.compute()\n    acc = metric_acc.compute()\n\n    # Print the loss for every epoch\n    print(f'Epoch {epoch+1}/{num_epochs}, Avg loss: {log_loss/num_it:.4f}, LR: {current_lr:.6f}, AUROC: {auroc:.4f}, Acc: {acc:.4f}')\n\n\n    metric = torchmetrics.classification.BinaryAUROC()\n    metric_acc = torchmetrics.classification.BinaryAccuracy()\n    metric.to(device)\n    metric_acc.to(device)\n    for inputs, labels in val_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            metric.update(outputs, labels)\n            metric_acc.update(outputs, labels)\n\n    auroc = metric.compute()\n    acc = metric_acc.compute()\n    torch.save(model, '/kaggle/working/clahe_model'+ str(epoch) +'.pt')\n    print(f'Epoch {epoch+1}/{num_epochs}, AUROC: {auroc:.4f}, Acc: {acc:.4f}')\n\nprint(f'Finished Training, Loss: {loss.item():.4f}')","metadata":{"is_executing":true,"execution":{"iopub.status.busy":"2024-04-23T13:48:59.246638Z","iopub.execute_input":"2024-04-23T13:48:59.246924Z","iopub.status.idle":"2024-04-23T13:57:58.386566Z","shell.execute_reply.started":"2024-04-23T13:48:59.246902Z","shell.execute_reply":"2024-04-23T13:57:58.385392Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/25, Avg loss: 57.6602, LR: 0.000010, AUROC: 0.6078, Acc: 0.4852\nEpoch 1/25, AUROC: 0.5053, Acc: 0.4769\nEpoch 2/25, Avg loss: 56.6223, LR: 0.000010, AUROC: 0.7185, Acc: 0.4904\nEpoch 2/25, AUROC: 0.5026, Acc: 0.4817\nEpoch 3/25, Avg loss: 56.0583, LR: 0.000010, AUROC: 0.7789, Acc: 0.4974\nEpoch 3/25, AUROC: 0.5032, Acc: 0.4777\nEpoch 4/25, Avg loss: 55.5148, LR: 0.000010, AUROC: 0.8273, Acc: 0.5016\nEpoch 4/25, AUROC: 0.4989, Acc: 0.4793\nEpoch 5/25, Avg loss: 54.8578, LR: 0.000010, AUROC: 0.8744, Acc: 0.5030\nEpoch 5/25, AUROC: 0.4993, Acc: 0.4825\nEpoch 6/25, Avg loss: 54.1274, LR: 0.000010, AUROC: 0.9152, Acc: 0.5225\nEpoch 6/25, AUROC: 0.5017, Acc: 0.4777\nEpoch 7/25, Avg loss: 53.3827, LR: 0.000010, AUROC: 0.9408, Acc: 0.5417\nEpoch 7/25, AUROC: 0.5013, Acc: 0.4825\nEpoch 8/25, Avg loss: 52.6452, LR: 0.000010, AUROC: 0.9619, Acc: 0.5708\nEpoch 8/25, AUROC: 0.5013, Acc: 0.4864\nEpoch 9/25, Avg loss: 51.8335, LR: 0.000010, AUROC: 0.9806, Acc: 0.6065\nEpoch 9/25, AUROC: 0.5022, Acc: 0.4888\nEpoch 10/25, Avg loss: 51.1149, LR: 0.000010, AUROC: 0.9882, Acc: 0.6613\nEpoch 10/25, AUROC: 0.5057, Acc: 0.4833\nEpoch 11/25, Avg loss: 50.5223, LR: 0.000010, AUROC: 0.9930, Acc: 0.7073\nEpoch 11/25, AUROC: 0.5056, Acc: 0.4864\nEpoch 12/25, Avg loss: 50.0076, LR: 0.000010, AUROC: 0.9968, Acc: 0.7586\nEpoch 12/25, AUROC: 0.5094, Acc: 0.4841\nEpoch 13/25, Avg loss: 49.5901, LR: 0.000010, AUROC: 0.9984, Acc: 0.8106\nEpoch 13/25, AUROC: 0.5071, Acc: 0.4880\nEpoch 14/25, Avg loss: 49.4144, LR: 0.000010, AUROC: 0.9984, Acc: 0.8297\nEpoch 14/25, AUROC: 0.5070, Acc: 0.4912\nEpoch 15/25, Avg loss: 49.1399, LR: 0.000010, AUROC: 0.9992, Acc: 0.8652\nEpoch 15/25, AUROC: 0.5082, Acc: 0.4793\nEpoch 16/25, Avg loss: 49.0197, LR: 0.000010, AUROC: 0.9994, Acc: 0.8792\nEpoch 16/25, AUROC: 0.5072, Acc: 0.4817\nEpoch 17/25, Avg loss: 48.9687, LR: 0.000010, AUROC: 0.9996, Acc: 0.8882\nEpoch 17/25, AUROC: 0.5048, Acc: 0.4864\nEpoch 18/25, Avg loss: 48.9049, LR: 0.000010, AUROC: 0.9997, Acc: 0.8947\nEpoch 18/25, AUROC: 0.5047, Acc: 0.4952\nEpoch 19/25, Avg loss: 48.8142, LR: 0.000010, AUROC: 0.9997, Acc: 0.9085\nEpoch 19/25, AUROC: 0.5055, Acc: 0.4848\nEpoch 20/25, Avg loss: 48.6510, LR: 0.000010, AUROC: 0.9998, Acc: 0.9149\nEpoch 20/25, AUROC: 0.5050, Acc: 0.4936\nEpoch 21/25, Avg loss: 48.6722, LR: 0.000010, AUROC: 0.9999, Acc: 0.9121\nEpoch 21/25, AUROC: 0.5017, Acc: 0.4841\nEpoch 22/25, Avg loss: 48.5435, LR: 0.000010, AUROC: 0.9999, Acc: 0.9290\nEpoch 22/25, AUROC: 0.5037, Acc: 0.4841\nEpoch 23/25, Avg loss: 48.4870, LR: 0.000010, AUROC: 0.9999, Acc: 0.9346\nEpoch 23/25, AUROC: 0.5046, Acc: 0.4841\nEpoch 24/25, Avg loss: 48.4505, LR: 0.000010, AUROC: 1.0000, Acc: 0.9376\nEpoch 24/25, AUROC: 0.5031, Acc: 0.4848\nEpoch 25/25, Avg loss: 48.4877, LR: 0.000010, AUROC: 0.9999, Acc: 0.9342\nEpoch 25/25, AUROC: 0.5053, Acc: 0.4809\nFinished Training, Loss: 35.3529\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/cropped_images_trained_modelResNet18.pt')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T13:57:58.388290Z","iopub.execute_input":"2024-04-23T13:57:58.388706Z","iopub.status.idle":"2024-04-23T13:57:58.465949Z","shell.execute_reply.started":"2024-04-23T13:57:58.388668Z","shell.execute_reply":"2024-04-23T13:57:58.465103Z"},"trusted":true},"execution_count":7,"outputs":[]}]}